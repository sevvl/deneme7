import requests
from bs4 import BeautifulSoup
import pandas as pd
import csv
import os
from datetime import datetime


def scrape_grape_disease_data(url, save_to_csv=True):
    """
    ÃœzÃ¼m hastalÄ±ÄŸÄ± fungisit verilerini Ã§eker

    Arg
        url (str): Ã‡ekilecek web sayfasÄ±nÄ±n URL'i
        save_to_csv (bool): Veriyi CSV'ye kaydet

    Returns:
        list: Ã‡ekilen veri listesi veya None
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
    }

    try:
        print("ğŸ”„ Web sayfasÄ±ndan veri Ã§ekiliyor...")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Tabloyu bul - farklÄ± seÃ§icileri dene
        table = None
        selectors = [
            {'class': 'tablestyle'},
            {'class': 'table'},
            {'id': 'fungicide-table'},
            None  # Herhangi bir tablo
        ]

        for selector in selectors:
            if selector:
                table = soup.find('table', selector)
            else:
                table = soup.find('table')

            if table:
                print(f"âœ… Tablo bulundu: {selector}")
                break

        if table:
            data = []
            rows = table.find_all('tr')

            if not rows:
                print("âŒ Tabloda satÄ±r bulunamadÄ±.")
                return None

            # Header'larÄ± Ã§Ä±kar
            header_row = rows[0]
            headers = []

            # Ã–nce th etiketlerini dene
            th_elements = header_row.find_all('th')
            if th_elements:
                headers = [th.get_text(strip=True, separator=' ') for th in th_elements]
            else:
                # th yoksa td'lerden header Ã§Ä±kar
                td_elements = header_row.find_all('td')
                if td_elements:
                    headers = [td.get_text(strip=True, separator=' ') for td in td_elements]

            # Header'larÄ± temizle
            headers = [h.replace('\n', ' ').replace('\r', ' ') for h in headers if h.strip()]

            if headers:
                data.append(headers)
                print(f"âœ… {len(headers)} sÃ¼tunlu header bulundu")

                # Veri satÄ±rlarÄ±nÄ± iÅŸle
                successful_rows = 0
                for row in rows[1:]:
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        row_data = []
                        for cell in cells:
                            # HÃ¼cre iÃ§eriÄŸini temizle
                            cell_text = cell.get_text(strip=True, separator=' ')
                            cell_text = cell_text.replace('\n', ' ').replace('\r', ' ')
                            row_data.append(cell_text)

                        # BoÅŸ olmayan satÄ±rlarÄ± ekle
                        if any(cell.strip() for cell in row_data) and len(row_data) >= len(headers):
                            # SÃ¼tun sayÄ±sÄ±nÄ± eÅŸitle
                            while len(row_data) < len(headers):
                                row_data.append('')
                            row_data = row_data[:len(headers)]  # Fazla sÃ¼tunlarÄ± kes

                            data.append(row_data)
                            successful_rows += 1

                print(f"âœ… {successful_rows} veri satÄ±rÄ± iÅŸlendi")

                # CSV'ye kaydet
                if save_to_csv and data:
                    try:
                        filename = 'grape_disease_data.csv'
                        with open(filename, 'w', newline='', encoding='utf-8') as f:
                            writer = csv.writer(f)
                            writer.writerows(data)
                        print(f"ğŸ’¾ Veri '{filename}' dosyasÄ±na kaydedildi")

                        # Metadata dosyasÄ± oluÅŸtur
                        metadata = {
                            'last_updated': datetime.now().isoformat(),
                            'source_url': url,
                            'total_rows': len(data),
                            'columns': len(headers) if headers else 0
                        }

                        with open('data_metadata.txt', 'w', encoding='utf-8') as f:
                            for key, value in metadata.items():
                                f.write(f"{key}: {value}\n")

                    except Exception as e:
                        print(f"âŒ CSV kaydetme hatasÄ±: {e}")

                return data
            else:
                print("âŒ Header bulunamadÄ±")
                return None
        else:
            print("âŒ Tablo bulunamadÄ±")
            return None

    except requests.exceptions.Timeout:
        print("âŒ Zaman aÅŸÄ±mÄ± - Web sitesi yavaÅŸ yanÄ±t veriyor")
        return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ä°stek hatasÄ±: {e}")
        return None
    except Exception as e:
        print(f"âŒ Beklenmeyen hata: {e}")
        return None


def load_cached_data():
    """
    Daha Ã¶nce kaydedilmiÅŸ CSV verilerini yÃ¼kle
    """
    try:
        if os.path.exists('grape_disease_data.csv'):
            df = pd.read_csv('grape_disease_data.csv')

            # Metadata kontrolÃ¼
            if os.path.exists('data_metadata.txt'):
                with open('data_metadata.txt', 'r', encoding='utf-8') as f:
                    metadata = f.read()
                    print(f"ğŸ“‹ Cached veri bilgisi:\n{metadata}")

            return df
        else:
            print("âŒ Ã–nbellek dosyasÄ± bulunamadÄ±")
            return None
    except Exception as e:
        print(f"âŒ Ã–nbellek yÃ¼kleme hatasÄ±: {e}")
        return None
import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_umass_fungicide_table():
    url = "https://www.umass.edu/agriculture-food-environment/fruit/ne-small-fruit-management-guide/grapes/diseases/table-54-effectiveness-of-fungicides-on-grape-diseases"

    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Hata: Sayfa alÄ±namadÄ±. {e}")
        return None

    # 1. Ä°lk olarak pandas ile tabloyu yakalamayÄ± dene
    try:
        dfs = pd.read_html(response.text)
        for df in dfs:
            if 'Phomopsis Cane and Leaf Spot' in df.columns:
                print("âœ… Tablo pandas.read_html ile baÅŸarÄ±yla bulundu.")
                return df
    except Exception as e:
        print(f"âŒ pandas.read_html ile tablo alÄ±namadÄ±: {e}")

    # 2. EÄŸer pandas ile olmadÄ±ysa, BeautifulSoup ile elle bul
    soup = BeautifulSoup(response.text, 'html.parser')
    table = None

    for t in soup.find_all('table'):
        caption = t.find('caption')
        if caption and 'Effectiveness of Fungicides' in caption.get_text():
            table = t
            print("âœ… Tablo BeautifulSoup ile caption Ã¼zerinden bulundu.")
            break
        elif 'Effectiveness of Fungicides' in t.get_text()[:300]:
            table = t
            print("âœ… Tablo BeautifulSoup ile metin taramasÄ±yla bulundu.")
            break

    if table:
        try:
            df = pd.read_html(str(table))[0]
            return df
        except Exception as e:
            print(f"âŒ Tablo pandas.read_html ile iÅŸlenemedi: {e}")
            return None

    print("âŒ Tablo bulunamadÄ±.")
    return None


# Bu fonksiyonu Ã§aÄŸÄ±rarak sonucu yazdÄ±r
if __name__ == "__main__":
    df = scrape_umass_fungicide_table()
    if df is not None:
        print(df.head())
        df.to_csv("fungicide_effectiveness.csv", index=False)
        print("âœ… Tablo 'fungicide_effectiveness.csv' olarak kaydedildi.")
    else:
        print("âŒ Veri Ã§ekilemedi.")


def get_grape_data_smart(force_refresh=False):
    """
    AkÄ±llÄ± veri Ã§ekme - Ã¶nce Ã¶nbelleÄŸi kontrol et, gerekirse yenile
    """
    url = "https://www.umass.edu/agriculture-food-environment/fruit/ne-small-fruit-management-guide/grapes/diseases/table-54-effectiveness-of-fungicides-on-grape-diseases"

    if not force_refresh:
        # Ã–nce Ã¶nbellekteki veriyi dene
        cached_df = load_cached_data()
        if cached_df is not None and not cached_df.empty:
            print("âœ… Ã–nbellekten veri yÃ¼klendi")
            return cached_df

    # Ã–nbellek yoksa veya yenileme isteniyorsa web'den Ã§ek
    print("ğŸŒ Web'den yeni veri Ã§ekiliyor...")
    raw_data = scrape_grape_disease_data(url)

    if raw_data and len(raw_data) > 1:
        try:
            df = pd.DataFrame(raw_data[1:], columns=raw_data[0])
            return df
        except Exception as e:
            print(f"âŒ DataFrame oluÅŸturma hatasÄ±: {e}")
            return None
    else:
        return None


if __name__ == "__main__":
    print("ğŸ‡ ÃœzÃ¼m HastalÄ±ÄŸÄ± Veri Ã‡ekici")
    print("=" * 40)

    # Test Ã§alÄ±ÅŸtÄ±rma
    df = get_grape_data_smart()

    if df is not None:
        print(f"\nğŸ“Š BaÅŸarÄ±! {len(df)} satÄ±r, {len(df.columns)} sÃ¼tun")
        print("SÃ¼tunlar:", list(df.columns))
        print("\nÄ°lk 3 satÄ±r:")
        print(df.head(3).to_string())
    else:
        print("âŒ Veri Ã§ekilemedi!")

    print("\n" + "=" * 40)
    print("Test tamamlandÄ±!")